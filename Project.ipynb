{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "TZxlVKwiJfo4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\selez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "import gzip\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "import regex\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.dummy import Pool, Queue\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from bs4.element import Comment\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UNQMQZe6fp86"
   },
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]', 'option']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "7Lol9U3tpGdK"
   },
   "outputs": [],
   "source": [
    "def get_title_and_text(doc_id):\n",
    "    \"\"\"\n",
    "        Get set of words from title\n",
    "        and most common words from text\n",
    "    \"\"\"\n",
    "    page = open(\"content/\" + str(doc_id) + \".dat\", 'r', encoding='utf-8').read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    title = soup.title.text\n",
    "\n",
    "    txt = soup.findAll(text=True)\n",
    "    visible = filter(tag_visible, txt)\n",
    "    txt = u\" \".join(t.strip() for t in visible)\n",
    "\n",
    "    reg = regex.compile('[^a-zA-Zа-яА-Я ]')\n",
    "    txt = reg.sub('', txt)\n",
    "    #if not hasattr(get_title_and_text, \"stemmer\"):\n",
    "    stemmer = SnowballStemmer(\"russian\")\n",
    "    stop = set(stopwords.words('russian'))\n",
    "    words = [stemmer.stem(word) for word in txt.split() if (not (word in stop)) and len(word) > 1]\n",
    "    #words = [get_title_and_text.stemmer.stem(word) for word in txt.split()\n",
    "    #         if len(word) > 3 or len(word) == 3 and word.isupper()]\n",
    "    cnt = Counter(words)\n",
    "\n",
    "    return (set(title.strip().split()), set(dict(cnt.most_common(50)).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "pGLEqMDYBaPk"
   },
   "outputs": [],
   "source": [
    "def process_page(pair_id):\n",
    "    \"\"\"\n",
    "        Get information about pair (doc, group):\n",
    "        - doc_id - номер документа,\n",
    "        - title - множество слов из заголовка страницы,\n",
    "        - text - множество из 50 самых частых слов текста страницы,\n",
    "        - target.\n",
    "    \"\"\"\n",
    "    new_doc = train_data.loc[pair_id]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title, text = get_title_and_text(doc_id)\n",
    "    #title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_data:\n",
    "        traingroups_data[doc_group] = []\n",
    "    traingroups_data[doc_group].append((doc_id, title, text, target))\n",
    "    return (doc_id, title, text, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "queue_groups = Queue()   # очередь ссылок на группы\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_data = {}\n",
    "train_data.index = train_data.pair_id\n",
    "\n",
    "groups_to_process = [2, 3, 4, 5]\n",
    "\n",
    "# Create queue of groups and n_groups queues of pair_ids in group\n",
    "queue = {i: Queue() for i in groups_to_process}# gtrain_data.group_id.unique()}\n",
    "for group in groups_to_process:# train_data.group_id.unique():\n",
    "    queue_groups.put(group)\n",
    "    for pair in train_data[train_data.group_id == group].itertuples():\n",
    "        queue[group].put(pair.pair_id)\n",
    "#for pair in train_data[train_data.group_id == 1].itertuples():\n",
    "#    queue.put(pair.pair_id)\n",
    "\n",
    "pages_data = [[] for i in groups_to_process]# train_data.group_id.unique()]\n",
    "\n",
    "zipfile = 'content.tar.gz'\n",
    "tar = tarfile.open(zipfile, \"r:gz\") # Считывает архив с данными\n",
    "\n",
    "def extract_pages(group):\n",
    "    \"\"\"\n",
    "        Extract pages data from .gz archive if not extracted\n",
    "    \"\"\"\n",
    "    for pair in train_data[train_data.group_id == group].itertuples():\n",
    "        if not os.path.exists(\"content/\" + str(pair.doc_id) + \".dat\"):\n",
    "            tar.extract(\"content/\" + str(pair.doc_id) + \".dat\")\n",
    "\n",
    "def process_page_wrapper(i):\n",
    "    while not queue_groups.empty():\n",
    "        group = int(queue_groups.get())\n",
    "        extract_pages(group)\n",
    "        with gzip.open('data/group_{:03d}.jsonl.gz'.format(group), mode='wb') as f_json:\n",
    "            f_json = codecs.getwriter('utf_16')(f_json)\n",
    "            record = {}\n",
    "            while not queue[group].empty():\n",
    "                pair_id = queue[group].get()\n",
    "                doc_id, title, text, target = process_page(pair_id)\n",
    "                record[pair_id] = {'doc_id': int(doc_id), 'title': list(title), 'text': list(text), 'target': int(target)}\n",
    "            record_str = json.dumps(record, indent=4, ensure_ascii=False)\n",
    "            print(record_str, file=f_json)\n",
    "            pages_data.append((doc_id, title, text, target))\n",
    "            with lock:\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "with Pool(processes=4) as pool, tqdm(total=queue_groups.qsize()) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    pool.map(process_page_wrapper, range(pool._processes))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 2 column 1 (char 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21324/2964436351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#data = json.loads(json.dumps(data))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#data = f_json.read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf_json\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#open('data.json', 'r')]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21324/2964436351.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#data = json.loads(json.dumps(data))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#data = f_json.read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf_json\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#open('data.json', 'r')]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 2 column 1 (char 3)"
     ]
    }
   ],
   "source": [
    "# Не нужно запускать!\n",
    "with gzip.open('data/part_{:05d}.jsonl.gz'.format(0), mode='rb') as f_json:\n",
    "    f_json = codecs.getreader('utf_16')(f_json)\n",
    "    #data = [{} for i in range(17)]\n",
    "    #data = json.loads(json.dumps(data))\n",
    "    #data = f_json.read()\n",
    "    data = [json.loads(line) for line in f_json]#open('data.json', 'r')]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "II6qlwgsqxLm"
   },
   "outputs": [],
   "source": [
    "def group_parser(group_data, group_id):\n",
    "    \"\"\"\n",
    "        Create features for documents in group_id\n",
    "    \"\"\"\n",
    "    for k, (doc_id, title, text, target_id) in enumerate(group_data):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(group_id)\n",
    "        all_dist_title = []\n",
    "        all_dist_text = []\n",
    "        #words = set(title.strip().split())\n",
    "        for j in range(0, len(group_data)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, text_j, target_j = group[j]\n",
    "            #words_j = set(title_j.strip().split())\n",
    "            all_dist_title.append(len(title.intersection(title_j)))\n",
    "            all_dist_text.append(len(text.intersection(text_j)))\n",
    "        X_train.append(sorted(all_dist_title, reverse=True)[0:15] + sorted(all_dist_text, reverse=True)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 30) (102,) (102,)\n"
     ]
    }
   ],
   "source": [
    "# Запуск создания фичей для одной группы -> переписать в цикл / многопроцессный код\n",
    "\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "group_id = 1\n",
    "group_parser(pages_data, group_id)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_score(clf.predict(X_train), y_train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
